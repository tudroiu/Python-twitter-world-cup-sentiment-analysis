{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#section One: Mining\n",
    "#this section of code denoted by a line of stars is used to access the host application, \n",
    "#in this case twitter and mine data we need. \n",
    "import tweepy\n",
    "from tweepy.streaming import StreamListener\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import Stream\n",
    "\n",
    "consumer_key = 'Insert your API Key'\n",
    "consumer_secret = 'Insert your API Secret'\n",
    "access_token = 'Insert your Access Token'\n",
    "access_secret = 'Insert your Access Token Secret'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/fifa2.json', 'a') as f:\n",
    "                f.write(data)\n",
    "            \n",
    "        except BaseException as e:\n",
    "            print(\"Error on data: %s\" % str(e))\n",
    "            pass\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    "\n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(languages=[\"en\"], track=['#FIFAWorldCup','FIFAWorldCup','#FiFaWorldCup2018'])\n",
    "\n",
    "##############################################################################################################\n",
    "#Section two, Cleaning,  this second section will both make a CSV which we will use to manilpulate our data. We will also clean for \n",
    "#certain words we have identified as non-purposeful text as these tweets contain links and images potentially.\n",
    "\n",
    "import json, pandas, nltk, re\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Open a file called pytweets in read mode \n",
    "with open('C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/fifa2.json', 'r') as file:\n",
    "    checklist = []\n",
    "    corpus = []\n",
    "    users = []\n",
    "    count = 0\n",
    "    location= []\n",
    "    # Iterate over each line in pytweets.json\n",
    "    for line in file:\n",
    "        # Check if the current tweet is already collected, if not then asssess it\n",
    "        if line not in checklist:\n",
    "            checklist.append(line)\n",
    "            # Count setting used to limit the tweets we take from the JSON file\n",
    "            # **** Set it to whatever you need ****\n",
    "            if count <= 100000:\n",
    "                try:\n",
    "                    # Set tweet to store each line as json - enabling us to call attributes\n",
    "                    tweet = json.loads(line)\n",
    "                    # Store tweet text in a variable\n",
    "                    tweettext = ascii(tweet['text'])\n",
    "                    # Append that variable to a list aka our corpus\n",
    "                    corpus.append(tweettext)\n",
    "                    # Do the same for the twitter username\n",
    "                    tweetuser = ascii(tweet['user']['screen_name'])\n",
    "                    users.append(tweetuser)\n",
    "                    loc  = ascii(tweet['user']['location'])\n",
    "                    location.append(loc)\n",
    "\n",
    "                    print(\"tweettext = \", tweettext)\n",
    "                except:\n",
    "                    # Pass statement is used to avoid any invalid json tweets\n",
    "                    # caused by stop-and-start streaming\n",
    "                    pass\n",
    "            # Count setting to stop the loop when we've hit our max number of tweets\n",
    "            # ***** Set it according to the setting above *****\n",
    "            elif count > 100000:\n",
    "                break\n",
    "        # If tweet is already in checklist then pass over it to avoid duplicates/spam tweets\n",
    "        if line in checklist:\n",
    "            pass\n",
    "            # Count setting to stop the loop when we've hit our max number of tweets\n",
    "        # If tweet is already in checklist then pass over it to avoid duplicates/spam tweets\n",
    "      \n",
    "    \n",
    "#this is a test to see our tweets and their \"clean\" counterpart.\n",
    "#Print the count of tweets you will analyze\n",
    "print(count, \"Tweets\\n\")\n",
    "#Print an example of raw tweet text\n",
    "print(\"7th Tweet:\",corpus[6])\n",
    "#Print the username associated with the tweet\n",
    "print(\"\\nTweeted by: \",users[6])\n",
    "\n",
    "#Clean our tweets by looping through each one in our corpus\n",
    "cleanlist = []\n",
    "for i in corpus:\n",
    "    #Remove new lines\n",
    "    value = i.replace(\"\\\\n\",\" \")\n",
    "    #Remove any https links in tweet\n",
    "    value = re.sub(r'http\\S+', ' ', value)\n",
    "    #Remove any \"&\" in tweet\n",
    "    value = value.replace(\"&amp;\",\" \")\n",
    "    #Remove any usernames from the tweet\n",
    "    value = re.sub(r'@\\S+', ' ', value)\n",
    "    #Remove special characters\n",
    "    value = re.sub(r'[.?!#$%^<>\\[\\]*+-=_:;\\\\)(\"|]', ' ', value)\n",
    "    #Remove emojis that have a code starting with \"\\u\"\n",
    "    value = re.sub(r'\\\\u\\S+', ' ', value)\n",
    "    #Remove emojis that have a code starting with \"\\U\"\n",
    "    value = re.sub(r'\\\\U\\S+', ' ', value)\n",
    "    #Remove any words containing a \"\\\" as they are also emoji codes\n",
    "    value = re.sub(r'\\S*\\\\\\S*', ' ', value)\n",
    "    #Remove any numbers in tweet\n",
    "    value = re.sub(r'[0-9]', ' ', value)\n",
    "    #Remove any apostrophes in tweet\n",
    "    value = value.replace(\"'\",\"\")\n",
    "    #Remove words that are 2 characters or less in length\n",
    "    value = re.sub(r'\\b\\w{1,2}\\b', ' ', value)\n",
    "    #Remove two or more spaces with just one\n",
    "    value = re.sub(r'\\s{2,}', ' ', value)\n",
    "    #Remove any leading spaces\n",
    "    value = re.sub(r\"^\\s+\" , \"\" , value)\n",
    "    #Append cleaned tweet to our list\n",
    "    cleanlist.append(value)\n",
    "\n",
    "#Print an example of a cleaned tweet\n",
    "print(\"\\n7th Tweet Cleaned up:\",cleanlist[7])\n",
    "\n",
    "#Set up list of data with username and cleaned tweet, so we can plug it into\n",
    "#our pandas dataframe\n",
    "tweet_Location = []\n",
    "x = 0\n",
    "for i in cleanlist:\n",
    "    username = users[x].replace(\"'\",\"\")\n",
    "    row = [username,i,location[x]]\n",
    "    tweet_Location.append(row)\n",
    "    x+=1\n",
    "\n",
    "tweetLocationcsv = pd.DataFrame(tweet_Location)\n",
    "tweetLocationcsv.columns=['user','tweet', 'location']\n",
    "tweetLocationcsv.to_csv(r'C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/tweetLocation.csv')\n",
    "\n",
    "#####################################################################################################################\n",
    "#section three:\n",
    "#using our previously created CSV we will use this to then find our insight\n",
    "# we must furtherclean our array/csv with regard to location. \n",
    "# then filter tweets for our chosen players: Ronaldo, Neymar, and Messi\n",
    "\n",
    "tweetLocationcsv = pd.read_csv(r'C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/tweetLocation.csv')\n",
    "\n",
    "#eliminate ' character\n",
    "tweetLocationcsv['location'] = tweetLocationcsv['location'].str.replace(\"'\", \"\")\n",
    "\n",
    "#split city/country into 2 new columns\n",
    "tweetLocationcsv = tweetLocationcsv.join((tweetLocationcsv['location'].str.rsplit(',', 1, expand=True)))\n",
    "\n",
    "#rename columns\n",
    "tweetLocationcsv.rename(columns={0: \"city\", 1: \"country\"}, inplace=True)\n",
    "\n",
    "#eliminate spaces from country column\n",
    "tweetLocationcsv['country'] = tweetLocationcsv['country'].str.strip()\n",
    "\n",
    "#Create list of Countries\n",
    "from  pycountry import countries\n",
    "listOfCountries=[]\n",
    "for item in countries:\n",
    "    listOfCountries.append(item.name.upper())\n",
    "\n",
    "#Make all countries Capital to make sure they match\n",
    "tweetLocationcsv['city'] = tweetLocationcsv['city'].str.upper()\n",
    "tweetLocationcsv['country'] = tweetLocationcsv['country'].str.upper()\n",
    "\n",
    "    \n",
    "#Move countries to country column and cities to city to city columns\n",
    "ix=0\n",
    "for citi in tweetLocationcsv['city']:\n",
    "    if citi in listOfCountries:\n",
    "        tweetLocationcsv.at[ix, 'country']   = citi\n",
    "    ix+=1\n",
    "\n",
    "#keep only tweets location that exist in the list of countries\n",
    "tweetLocationcsv = tweetLocationcsv[tweetLocationcsv['country'].isin(listOfCountries)]\n",
    "\n",
    "#Visualize count of tweets by country (TOP 25)\n",
    "import seaborn as sns\n",
    "tweetLocationcsv['country'].value_counts().head(25).plot.bar()\n",
    "\n",
    "#make all tweets lowercase to make sure it matches the search\n",
    "tweetLocationcsv['tweet'] = tweetLocationcsv['tweet'].str.lower()\n",
    "\n",
    "#found some records where tweet is empty\n",
    "tweetLocationcsv= tweetLocationcsv.dropna()\n",
    "\n",
    "#******************************************************************************************\n",
    "# TWEETS ABOUT PLAYERS PER COUNTRY\n",
    "#create new dataframe only with tweets about messi\n",
    "messiTweets =  tweetLocationcsv[tweetLocationcsv['tweet'].str.contains(\"messi\")]\n",
    "ronaldoTweets =  tweetLocationcsv[tweetLocationcsv['tweet'].str.contains(\"ronaldo\")]\n",
    "neymarTweets =  tweetLocationcsv[tweetLocationcsv['tweet'].str.contains(\"neymar\")]\n",
    "\n",
    "messiTweets[['tweet', 'country']].groupby(['country']).agg(['count']).plot.bar().set_title(\"All tweets about Messi\")\n",
    "ronaldoTweets[['tweet', 'country']].groupby(['country']).agg(['count']).plot.bar().set_title(\"All tweets about Ronaldo\")\n",
    "neymarTweets[['tweet', 'country']].groupby(['country']).agg(['count']).plot.bar().set_title(\"All tweets about Neymar\")\n",
    "\n",
    "#Section Four: Train our classisfier\n",
    "#******************************************************************************************\n",
    "#******************************************************************************************\n",
    "#******************************************************************************************\n",
    "#******************************************************************************************\n",
    "#******************************************************************************************\n",
    "#POSITIVE/NEGATIVE TWEETS ABOUT PLAYERS PER COUNTRY\n",
    "\n",
    "#these next 2 csv files are files found  online created by a 3rd party to train the model for pos & neg\n",
    "#they are included in the Zip folder with these filenames, the path must be adjusted to you location\n",
    "trainingDataPos = pd.read_csv(r'C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/trainingdataPos.csv')\n",
    "trainingDataNeg = pd.read_csv(r'C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/trainingdataNeg.csv')\n",
    "\n",
    "trainingDataPos = trainingDataPos.drop(columns=['a','b', 'c', 'user'])\n",
    "trainingDataNeg = trainingDataNeg.drop(columns=['a','b', 'c', 'user'])\n",
    "\n",
    "#converting 0 to Negative and 4 to Positive\n",
    "trainingDataNeg['sentiment'] = trainingDataNeg['sentiment'].map({0:'Negative'})\n",
    "trainingDataPos['sentiment'] = trainingDataPos['sentiment'].map({4:'Positive'})\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "#Convert DataFrame to List\n",
    "trainingDataPosList = []\n",
    "for pair in trainingDataPos.values:\n",
    "    trainingDataPosList.append([pair[1], pair[0]])\n",
    "trainingDataNegList = []\n",
    "for pair in trainingDataNeg.values:\n",
    "    trainingDataNegList.append([pair[1], pair[0]])\n",
    "    \n",
    "#make dicitionary from classifier\n",
    "\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words.split() ])\n",
    "\n",
    "positiveWords=[]\n",
    "for line in trainingDataPosList:\n",
    "    positiveWords = positiveWords + [(word_feats(f), 'Positive') for f in line[0].split() if line[1] =='Positive']\n",
    "\n",
    "negativeWords=[]\n",
    "for line in trainingDataNegList:\n",
    "    negativeWords = negativeWords + [(word_feats(f), 'Negative') for f in line[0].split() if line[1] =='Negative']\n",
    "\n",
    "trainingTweets = positiveWords + negativeWords\n",
    "\n",
    "classifier =nltk.NaiveBayesClassifier.train(trainingTweets)\n",
    "\n",
    "\n",
    "#Analize Messi Tweets and add sentiment to DataFrame\n",
    "#add empty column to hold sentiment\n",
    "\n",
    "messiNegativeCounts=0\n",
    "messiPositiveCounts=0\n",
    "for mtweet in messiTweets.values:\n",
    "    if classifier.classify(word_feats(mtweet[2])) == 'Positive':\n",
    "        messiTweets.at[mtweet[0], 'sentiment'] = 'Positive'\n",
    "        messiPositiveCounts+=1\n",
    "    else:\n",
    "        messiTweets.at[mtweet[0], 'sentiment'] = 'Negative'\n",
    "        messiNegativeCounts+=1\n",
    "\n",
    "#Analize Ronaldo Tweets and add sentiment to DataFrame\n",
    "#add empty column to hold sentiment\n",
    "ronaldoNegativeCounts=0\n",
    "ronaldoPositiveCounts=0\n",
    "for rtweet in ronaldoTweets.values:\n",
    "    if classifier.classify(word_feats(rtweet[2])) == 'Positive':\n",
    "        ronaldoTweets.at[rtweet[0], 'sentiment'] = 'Positive'\n",
    "        ronaldoPositiveCounts+=1\n",
    "    else:\n",
    "        ronaldoTweets.at[rtweet[0], 'sentiment'] = 'Negative'\n",
    "        ronaldoNegativeCounts+=1\n",
    "\n",
    "\n",
    "#Analize Neymar Tweets and add sentiment to DataFrame\n",
    "#add empty column to hold sentiment\n",
    "neymarNegativeCounts=0\n",
    "neymarPositiveCounts=0\n",
    "for ntweet in neymarTweets.values:\n",
    "    if classifier.classify(word_feats(ntweet[2])) == 'Positive':\n",
    "        neymarTweets.at[ntweet[0], 'sentiment'] = 'Positive'\n",
    "        neymarPositiveCounts+=1\n",
    "    else:\n",
    "        neymarTweets.at[ntweet[0], 'sentiment'] = 'Negative'\n",
    "        neymarNegativeCounts+=1\n",
    "\n",
    "messiTotalTweets = messiTweets['tweet'].count()\n",
    "ronaldoTotalTweets = ronaldoTweets['tweet'].count()\n",
    "neymarTotalTweets = neymarTweets['tweet'].count()\n",
    "\n",
    "#Secction 5: Sentiment analysis!\n",
    "\n",
    "print(\"\\n\",messiPositiveCounts, \"Positive Tweets for Messi out of \",messiTotalTweets, \" for \", \"{0:2f}\".format((messiPositiveCounts/messiTotalTweets)*100), \"% of tweets where he is mentioned\")\n",
    "print(\"\\n\",ronaldoPositiveCounts, \"Positive Tweets for Ronaldo out of \",ronaldoTotalTweets, \" for \", \"{0:2f}\".format((ronaldoPositiveCounts/ronaldoTotalTweets)*100), \"% of tweets where he is mentioned\")\n",
    "print(\"\\n\",neymarPositiveCounts, \"Positive Tweets for Neymar out of \",neymarTotalTweets, \" for \", \"{0:2f}\".format((neymarPositiveCounts/neymarTotalTweets)*100), \"% of tweets where he is mentioned\")\n",
    "    \n",
    "#PLOT Positive/Negative tweets comparisson by Country\n",
    "messiTweets[['tweet', 'country', 'sentiment']].groupby(['country', 'sentiment']).agg(['count']).plot.bar()\n",
    "ronaldoTweets[['tweet', 'country', 'sentiment']].groupby(['country', 'sentiment']).agg(['count']).plot.bar()\n",
    "neymarTweets[['tweet', 'country', 'sentiment']].groupby(['country', 'sentiment']).agg(['count']).plot.bar()\n",
    "\n",
    "#Plot country with most positive tweets\n",
    "messiTweets[messiTweets['sentiment'] == 'Positive'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "messiTweets[messiTweets['sentiment'] == 'Negative'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "\n",
    "#Plot country with most positive tweets (Ignoring INDIA)\n",
    "messiTweets[messiTweets['sentiment'] == 'Positive'][messiTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Positive Tweets about Messi\")\n",
    "messiTweets[messiTweets['sentiment'] == 'Negative'][messiTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Negative Tweets about Messi\")\n",
    "\n",
    "#Plot country with most positive tweets\n",
    "neymarTweets[neymarTweets['sentiment'] == 'Positive'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "neymarTweets[neymarTweets['sentiment'] == 'Negative'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "\n",
    "#Plot country with most positive tweets (Ignoring INDIA)\n",
    "neymarTweets[neymarTweets['sentiment'] == 'Positive'][neymarTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Positive Tweets about Neymar\")\n",
    "neymarTweets[neymarTweets['sentiment'] == 'Negative'][neymarTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Negative Tweets about Neymar\")\n",
    "\n",
    "#Plot country with most positive tweets\n",
    "ronaldoTweets[ronaldoTweets['sentiment'] == 'Positive'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "ronaldoTweets[ronaldoTweets['sentiment'] == 'Negative'].groupby(['sentiment', 'country']).agg(['count']).plot.bar()\n",
    "\n",
    "#Plot country with most positive tweets (Ignoring INDIA)\n",
    "ronaldoTweets[ronaldoTweets['sentiment'] == 'Positive'][ronaldoTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Positive Tweets about Ronaldo\")\n",
    "ronaldoTweets[ronaldoTweets['sentiment'] == 'Negative'][ronaldoTweets['country'] != 'INDIA'].groupby(['sentiment', 'country']).agg(['count']).plot.bar().set_title(\"Negative Tweets about Ronaldo\")\n",
    "\n",
    "#TEST ACCURACY ************************************************************************************************************\n",
    "#*********************************************************************************************************************\n",
    "\n",
    "def feature_Extractor(tweet):\n",
    "    \"\"\"\n",
    "    Takes a tweet and extracts its features\n",
    "    \"\"\"\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in featureList:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features\n",
    "\n",
    "testdata  = pd.read_csv(r'C:/Users/Tudor/Desktop/BDA 102/Assignments/Assignment 4/testdatamanual20090614.csv')\n",
    "test_data1=[]\n",
    "for line in testdata:\n",
    "    test_data = negativeWords + [(word_feats(f), 'Negative') for f in line[0].split() if line[1] =='Negative']\n",
    "test_data2=[]\n",
    "for line in testdata:\n",
    "    test_data2 = negativeWords + [(word_feats(f), 'Positive') for f in line[0].split() if line[1] =='Positive']\n",
    "\n",
    "test_data = test_data1+test_data2\n",
    "\n",
    "testing_set = nltk.classify.apply_features(feature_Extractor, test_data)\n",
    "\n",
    "accuracy = nltk.classify.accuracy(classifier, test_data)*100\n",
    "\n",
    "print('Accuracy: {:4.2f}'.format(nltk.classify.accuracy(classifier, test_data)))\n",
    "\n",
    "\n",
    "#END\n",
    "#*********************************************************************************************************************"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
